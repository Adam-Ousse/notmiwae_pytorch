{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08dd1e44",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e7a4ade",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'notmiwae_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Import our not-MIWAE implementation\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnotmiwae_pytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NotMIWAE, MIWAE, Trainer\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnotmiwae_pytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     set_seed, \n\u001b[0;32m     17\u001b[0m     imputation_rmse, \n\u001b[0;32m     18\u001b[0m     introduce_mnar_missing,\n\u001b[0;32m     19\u001b[0m     standardize\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Set random seed for reproducibility\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'notmiwae_pytorch'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the path so we can import our notmiwae_pytorch module\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our not-MIWAE implementation\n",
    "from notmiwae_pytorch import NotMIWAE, MIWAE, Trainer\n",
    "from notmiwae_pytorch.utils import (\n",
    "    set_seed, \n",
    "    imputation_rmse, \n",
    "    introduce_mnar_missing,\n",
    "    standardize\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3bc03d",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "We'll use the UCI Wine Quality dataset, following the experimental setup in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d44353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wine Quality dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "data = pd.read_csv(url, sep=';')\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nColumn names: {list(data.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b22aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the quality column (target) - we only want features\n",
    "X = data.drop('quality', axis=1).values.astype(np.float32)\n",
    "N, D = X.shape\n",
    "\n",
    "print(f\"Data shape: N={N} samples, D={D} features\")\n",
    "\n",
    "# Standardize the data\n",
    "X_std, mean, std = standardize(X)\n",
    "\n",
    "# Random permutation\n",
    "perm = np.random.permutation(N)\n",
    "X_std = X_std[perm]\n",
    "\n",
    "print(f\"\\nData statistics after standardization:\")\n",
    "print(f\"Mean: {X_std.mean(axis=0).round(4)}\")\n",
    "print(f\"Std: {X_std.std(axis=0).round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db8bbc",
   "metadata": {},
   "source": [
    "## 2. Introduce Missing Values (MNAR Mechanism)\n",
    "\n",
    "Following the paper's experimental setup, we introduce **self-masking MNAR**:\n",
    "- In the first D/2 dimensions, values above the mean are missing\n",
    "- This creates a challenging scenario where the missingness depends on the values themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe24eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce MNAR missing values\n",
    "X_nan, X_filled, mask = introduce_mnar_missing(X_std)\n",
    "\n",
    "print(f\"Missing value statistics:\")\n",
    "print(f\"Total missing rate: {(1 - mask.mean()):.2%}\")\n",
    "print(f\"Missing rate per feature:\")\n",
    "for i, rate in enumerate(1 - mask.mean(axis=0)):\n",
    "    print(f\"  Feature {i}: {rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5bd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the missing pattern\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap of missing values (sample of first 100 rows)\n",
    "n_show = min(100, mask.shape[0])\n",
    "im = axes[0].imshow(mask[:n_show], aspect='auto', cmap='RdYlGn', interpolation='nearest')\n",
    "axes[0].set_xlabel('Feature')\n",
    "axes[0].set_ylabel('Sample')\n",
    "axes[0].set_title(f'Missing Pattern (1=observed, 0=missing)\\nFirst {n_show} samples')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Missing rate per feature\n",
    "missing_rates = 1 - mask.mean(axis=0)\n",
    "axes[1].bar(range(len(missing_rates)), missing_rates)\n",
    "axes[1].set_xlabel('Feature Index')\n",
    "axes[1].set_ylabel('Missing Rate')\n",
    "axes[1].set_title('Missing Rate by Feature')\n",
    "axes[1].axhline(y=missing_rates.mean(), color='r', linestyle='--', label=f'Mean: {missing_rates.mean():.2%}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba022bb",
   "metadata": {},
   "source": [
    "## 3. Create Datasets and DataLoaders\n",
    "\n",
    "We'll split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666e21c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/validation split\n",
    "train_ratio = 0.8\n",
    "n_train = int(N * train_ratio)\n",
    "\n",
    "# Training data\n",
    "X_train_filled = X_filled[:n_train]\n",
    "mask_train = mask[:n_train]\n",
    "X_train_original = X_std[:n_train]\n",
    "\n",
    "# Validation data\n",
    "X_val_filled = X_filled[n_train:]\n",
    "mask_val = mask[n_train:]\n",
    "X_val_original = X_std[n_train:]\n",
    "\n",
    "print(f\"Training samples: {n_train}\")\n",
    "print(f\"Validation samples: {N - n_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017159e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_filled_t = torch.tensor(X_train_filled, dtype=torch.float32)\n",
    "mask_train_t = torch.tensor(mask_train, dtype=torch.float32)\n",
    "X_train_original_t = torch.tensor(X_train_original, dtype=torch.float32)\n",
    "\n",
    "X_val_filled_t = torch.tensor(X_val_filled, dtype=torch.float32)\n",
    "mask_val_t = torch.tensor(mask_val, dtype=torch.float32)\n",
    "X_val_original_t = torch.tensor(X_val_original, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDatasets (x_filled, mask, x_original)\n",
    "train_dataset = TensorDataset(X_train_filled_t, mask_train_t, X_train_original_t)\n",
    "val_dataset = TensorDataset(X_val_filled_t, mask_val_t, X_val_original_t)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1247e3",
   "metadata": {},
   "source": [
    "## 4. Create and Train the not-MIWAE Model\n",
    "\n",
    "We'll train the not-MIWAE model with the `selfmasking_known` missing process, which assumes we know that higher values are more likely to be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e8d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "latent_dim = D - 1  # Following the paper's setup\n",
    "hidden_dim = 128\n",
    "n_samples = 20  # Number of importance samples\n",
    "\n",
    "# Create the not-MIWAE model\n",
    "notmiwae = NotMIWAE(\n",
    "    input_dim=D,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_samples=n_samples,\n",
    "    out_dist='gauss',\n",
    "    missing_process='selfmasking_known'  # Key: we use the known self-masking mechanism\n",
    ")\n",
    "\n",
    "print(f\"not-MIWAE Model:\")\n",
    "print(f\"  Input dimension: {D}\")\n",
    "print(f\"  Latent dimension: {latent_dim}\")\n",
    "print(f\"  Hidden dimension: {hidden_dim}\")\n",
    "print(f\"  Number of importance samples: {n_samples}\")\n",
    "print(f\"  Missing process: selfmasking_known\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in notmiwae.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7435a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainer with original_data_available=True to track RMSE during training\n",
    "trainer_notmiwae = Trainer(\n",
    "    model=notmiwae,\n",
    "    lr=1e-3,\n",
    "    device=device,\n",
    "    log_dir='./runs',\n",
    "    checkpoint_dir='./checkpoints',\n",
    "    original_data_available=True,  # Track imputation RMSE each epoch\n",
    "    rmse_n_samples=50  # Fewer samples for speed during training\n",
    ")\n",
    "\n",
    "print(f\"Trainer created. Device: {trainer_notmiwae.device}\")\n",
    "print(f\"TensorBoard logs will be saved to: {trainer_notmiwae.log_dir}\")\n",
    "print(f\"Tracking imputation RMSE during training: {trainer_notmiwae.original_data_available}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2cf6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the model graph to TensorBoard before training\n",
    "# This allows you to visualize the model architecture in TensorBoard's \"Graphs\" tab\n",
    "\n",
    "# Get a sample batch to use for graph logging\n",
    "sample_batch = next(iter(train_loader))\n",
    "x_sample, s_sample, _ = sample_batch\n",
    "\n",
    "# Manually trigger graph logging\n",
    "trainer_notmiwae.log_model_graph(x_sample, s_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the not-MIWAE model\n",
    "print(\"Training not-MIWAE...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history_notmiwae = trainer_notmiwae.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    n_epochs=200,\n",
    "    log_interval=20,\n",
    "    save_best=True,\n",
    "    early_stopping_patience=30,\n",
    "    checkpoint_name='notmiwae_best.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb44124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history including RMSE\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history_notmiwae['train_loss'], label='Train')\n",
    "if history_notmiwae.get('val_loss'):\n",
    "    axes[0].plot(history_notmiwae['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (-ELBO)')\n",
    "axes[0].set_title('not-MIWAE Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# ELBO curves\n",
    "axes[1].plot(history_notmiwae['train_elbo'], label='Train')\n",
    "if history_notmiwae.get('val_elbo'):\n",
    "    axes[1].plot(history_notmiwae['val_elbo'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('ELBO')\n",
    "axes[1].set_title('not-MIWAE ELBO')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE curves\n",
    "if history_notmiwae.get('train_rmse'):\n",
    "    axes[2].plot(history_notmiwae['train_rmse'], label='Train')\n",
    "    if history_notmiwae.get('val_rmse'):\n",
    "        axes[2].plot(history_notmiwae['val_rmse'], label='Validation')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('RMSE')\n",
    "    axes[2].set_title('not-MIWAE Imputation RMSE')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('notmiwae_training.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print final RMSE\n",
    "if history_notmiwae.get('val_rmse'):\n",
    "    print(f\"\\nFinal Validation RMSE: {history_notmiwae['val_rmse'][-1]:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342daf8",
   "metadata": {},
   "source": [
    "## 5. Train the Standard MIWAE for Comparison\n",
    "\n",
    "For comparison, we'll also train the standard MIWAE which doesn't model the missing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d3386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MIWAE model (without missing process modeling)\n",
    "miwae = MIWAE(\n",
    "    input_dim=D,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_samples=n_samples,\n",
    "    out_dist='gauss'\n",
    ")\n",
    "\n",
    "print(f\"MIWAE Model (no missing process):\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in miwae.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer for MIWAE with RMSE tracking\n",
    "trainer_miwae = Trainer(\n",
    "    model=miwae,\n",
    "    lr=1e-3,\n",
    "    device=device,\n",
    "    log_dir='./runs',\n",
    "    checkpoint_dir='./checkpoints',\n",
    "    original_data_available=True,\n",
    "    rmse_n_samples=50\n",
    ")\n",
    "\n",
    "# Log the MIWAE model graph to TensorBoard\n",
    "trainer_miwae.log_model_graph(x_sample, s_sample)\n",
    "\n",
    "\n",
    "# Train MIWAE\n",
    "print(\"\\nTraining MIWAE...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history_miwae = trainer_miwae.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    n_epochs=200,\n",
    "    log_interval=20,\n",
    "    save_best=True,\n",
    "    early_stopping_patience=30,\n",
    "    checkpoint_name='miwae_best.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dc4b6c",
   "metadata": {},
   "source": [
    "## 6. Evaluate Imputation Performance\n",
    "\n",
    "Now let's compare the imputation performance of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ccbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best models\n",
    "trainer_notmiwae.load_checkpoint('notmiwae_best.pt')\n",
    "trainer_miwae.load_checkpoint('miwae_best.pt')\n",
    "\n",
    "# Compute imputation RMSE\n",
    "n_imp_samples = 1000  # More samples for better imputation\n",
    "\n",
    "print(\"Computing imputation RMSE...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab49713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not-MIWAE imputation\n",
    "rmse_notmiwae, X_imputed_notmiwae = imputation_rmse(\n",
    "    model=notmiwae,\n",
    "    x_original=torch.tensor(X_train_original),\n",
    "    x_filled=torch.tensor(X_train_filled),\n",
    "    mask=torch.tensor(mask_train),\n",
    "    n_samples=n_imp_samples,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nnot-MIWAE Imputation RMSE: {rmse_notmiwae:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d1416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIWAE imputation\n",
    "rmse_miwae, X_imputed_miwae = imputation_rmse(\n",
    "    model=miwae,\n",
    "    x_original=torch.tensor(X_train_original),\n",
    "    x_filled=torch.tensor(X_train_filled),\n",
    "    mask=torch.tensor(mask_train),\n",
    "    n_samples=n_imp_samples,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"\\nMIWAE Imputation RMSE: {rmse_miwae:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d59545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean imputation baseline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed_mean = imputer.fit_transform(X_nan[:n_train])\n",
    "\n",
    "# Compute RMSE for mean imputation\n",
    "missing_mask = (1 - mask_train).astype(bool)\n",
    "rmse_mean = np.sqrt(np.mean((X_train_original[missing_mask] - X_imputed_mean[missing_mask])**2))\n",
    "\n",
    "print(f\"Mean Imputation RMSE: {rmse_mean:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a53d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPUTATION RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Method               | RMSE\")\n",
    "print(\"-\"*40)\n",
    "print(f\"Mean Imputation      | {rmse_mean:.5f}\")\n",
    "print(f\"MIWAE                | {rmse_miwae:.5f}\")\n",
    "print(f\"not-MIWAE            | {rmse_notmiwae:.5f}\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "improvement = (rmse_miwae - rmse_notmiwae) / rmse_miwae * 100\n",
    "print(f\"\\nnot-MIWAE improvement over MIWAE: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aacf64",
   "metadata": {},
   "source": [
    "## 7. Interpret the Missing Process\n",
    "\n",
    "One unique feature of not-MIWAE is the ability to interpret the learned missing process. We can analyze which features influence the probability of missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b57731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names from the dataset\n",
    "feature_names = list(data.drop('quality', axis=1).columns)\n",
    "\n",
    "# Create a new model with feature names for interpretation\n",
    "notmiwae_interp = NotMIWAE(\n",
    "    input_dim=D,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_samples=n_samples,\n",
    "    out_dist='gauss',\n",
    "    missing_process='selfmasking_known',\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "# Load the trained weights\n",
    "notmiwae_interp.load_state_dict(notmiwae.state_dict())\n",
    "notmiwae_interp.eval()\n",
    "\n",
    "# Interpret the missing process\n",
    "interpretation = notmiwae_interp.interpret_missing_process()\n",
    "print(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9618ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing process coefficients\n",
    "import torch.nn.functional as F\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# For selfmasking_known, we have per-feature coefficients\n",
    "# The missing process uses logit(p(s=1|x)) = -W * (x - b)\n",
    "# Access through missing_model (not missing_process)\n",
    "W = F.softplus(notmiwae_interp.missing_model.W).detach().squeeze().cpu().numpy()\n",
    "b = notmiwae_interp.missing_model.b.detach().squeeze().cpu().numpy()\n",
    "\n",
    "# Plot W coefficients for each feature\n",
    "x_pos = np.arange(D)\n",
    "colors = ['red' if w > 0.5 else 'blue' for w in W]\n",
    "\n",
    "bars = ax.bar(x_pos, W, color=colors, alpha=0.7)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "ax.set_xlabel('Feature')\n",
    "ax.set_ylabel('Missing Process Weight (W)')\n",
    "ax.set_title('Missing Process Coefficients\\n(Higher W = stronger self-masking effect)')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('missing_process_interpretation.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: In self-masking MNAR, the model learns: logit(p(s=1|x)) = -W*(x-b)\")\n",
    "print(\"Larger W means higher values are more likely to be missing.\")\n",
    "print(f\"The first {D//2} features were made MNAR in our simulation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c23def",
   "metadata": {},
   "source": [
    "## 8. Visualize Imputation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a91ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize imputation for a specific feature\n",
    "feature_idx = 0  # First feature (has MNAR missing values)\n",
    "missing_idx = mask_train[:, feature_idx] == 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Sample indices for visualization\n",
    "n_show = min(50, missing_idx.sum())\n",
    "sample_indices = np.where(missing_idx)[0][:n_show]\n",
    "\n",
    "# Plot original values, filled values, and imputed values\n",
    "x_pos = np.arange(n_show)\n",
    "width = 0.2  # Narrower bars to fit 4 groups\n",
    "\n",
    "ax.bar(x_pos - 1.5*width, X_train_original[sample_indices, feature_idx], width, label='Original (true)', alpha=0.8, color='C0')\n",
    "ax.bar(x_pos - 0.5*width, X_train_filled[sample_indices, feature_idx], width, label='Filled (mean)', alpha=0.8, color='C1')\n",
    "ax.bar(x_pos + 0.5*width, X_imputed_miwae[sample_indices, feature_idx], width, label='MIWAE imputed', alpha=0.8, color='C2')\n",
    "ax.bar(x_pos + 1.5*width, X_imputed_notmiwae[sample_indices, feature_idx], width, label='not-MIWAE imputed', alpha=0.8, color='C3')\n",
    "\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title(f'Imputation Comparison for Feature {feature_idx}\\n(showing {n_show} missing values)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('imputation_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d38bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare imputation methods for a specific feature\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "feature_idx = 0  # Feature with MNAR missing values\n",
    "missing_idx = mask_train[:, feature_idx] == 0\n",
    "\n",
    "# Scatter plot: Original vs Imputed\n",
    "axes[0].scatter(X_train_original[missing_idx, feature_idx], \n",
    "                X_imputed_mean[missing_idx, feature_idx], \n",
    "                alpha=0.5, label='Mean')\n",
    "axes[0].scatter(X_train_original[missing_idx, feature_idx], \n",
    "                X_imputed_miwae[missing_idx, feature_idx], \n",
    "                alpha=0.5, label='MIWAE')\n",
    "axes[0].scatter(X_train_original[missing_idx, feature_idx], \n",
    "                X_imputed_notmiwae[missing_idx, feature_idx], \n",
    "                alpha=0.5, label='not-MIWAE')\n",
    "\n",
    "# Perfect imputation line\n",
    "lim = [X_train_original[missing_idx, feature_idx].min(), \n",
    "       X_train_original[missing_idx, feature_idx].max()]\n",
    "axes[0].plot(lim, lim, 'k--', label='Perfect')\n",
    "\n",
    "axes[0].set_xlabel('Original Value')\n",
    "axes[0].set_ylabel('Imputed Value')\n",
    "axes[0].set_title(f'Imputation Quality (Feature {feature_idx})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution\n",
    "errors_mean = X_imputed_mean[missing_idx, feature_idx] - X_train_original[missing_idx, feature_idx]\n",
    "errors_miwae = X_imputed_miwae[missing_idx, feature_idx] - X_train_original[missing_idx, feature_idx]\n",
    "errors_notmiwae = X_imputed_notmiwae[missing_idx, feature_idx] - X_train_original[missing_idx, feature_idx]\n",
    "\n",
    "axes[1].hist(errors_mean, bins=30, alpha=0.5, label=f'Mean (bias={errors_mean.mean():.3f})')\n",
    "axes[1].hist(errors_miwae, bins=30, alpha=0.5, label=f'MIWAE (bias={errors_miwae.mean():.3f})')\n",
    "axes[1].hist(errors_notmiwae, bins=30, alpha=0.5, label=f'not-MIWAE (bias={errors_notmiwae.mean():.3f})')\n",
    "axes[1].axvline(x=0, color='k', linestyle='--')\n",
    "axes[1].set_xlabel('Imputation Error')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Error Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "# Box plot of absolute errors\n",
    "abs_errors = [np.abs(errors_mean), np.abs(errors_miwae), np.abs(errors_notmiwae)]\n",
    "bp = axes[2].boxplot(abs_errors, labels=['Mean', 'MIWAE', 'not-MIWAE'])\n",
    "axes[2].set_ylabel('Absolute Error')\n",
    "axes[2].set_title('Absolute Error Distribution')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('imputation_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
